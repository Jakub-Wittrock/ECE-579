{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "885f5451",
   "metadata": {},
   "source": [
    "# Multi-Input Neural Network "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2585e087",
   "metadata": {},
   "source": [
    "In this notebook we will experiment with neural networks that can handle multiple types of input data. We will use TensorFlow and Keras to build a model that takes both numerical and image data as inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7963936a",
   "metadata": {},
   "source": [
    "## Librosa Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b617e23",
   "metadata": {},
   "source": [
    "The motivation behind this notebook is to create a neural network that can take both numerical data (e.g., features extracted from audio files using Librosa) and image data (e.g., spectrograms or other visual representations of audio) as inputs. This can be useful in scenarios where we want to leverage both types of information for tasks such as classification or regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "063b26fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a008f3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "LIBROSA_DATA = r\"C:\\Users\\JTWit\\Documents\\ECE 579\\Datasets\\GTZAN Dataset\\features_30_sec.csv\"\n",
    "BASE_PATH = r\"C:\\Users\\JTWit\\Documents\\ECE 579\\Datasets\\Split GTZAN Dataset\"\n",
    "TEST_PATH = os.path.join(BASE_PATH,'test')\n",
    "TRAIN_PATH = os.path.join(BASE_PATH,'train')\n",
    "\n",
    "UN_SPLIT_PATH = r\"C:\\Users\\JTWit\\Documents\\ECE 579\\Datasets\\GTZAN Dataset\\images_original\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0f1822",
   "metadata": {},
   "source": [
    "Let's load the librosa data using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5581390",
   "metadata": {},
   "outputs": [],
   "source": [
    "librosa_df = pd.read_csv(LIBROSA_DATA)\n",
    "\n",
    "librosa_df[\"filename\"] = librosa_df[\"filename\"].str.replace(\".\", \"\",1, regex=False)\n",
    "librosa_df[\"filename\"] = librosa_df[\"filename\"].str.replace(\".wav\",'.png')\n",
    "\n",
    "FEATURE_COLS = librosa_df.columns.difference([\"filename\", \"label\"])\n",
    "librosa_df[\"label_id\"] = librosa_df[\"label\"].astype(\"category\").cat.codes\n",
    "\n",
    "train_df, val_df = train_test_split(\n",
    "    librosa_df,\n",
    "    test_size=0.2,\n",
    "    stratify=librosa_df[\"label_id\"],  # keeps class balance\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "def build_image_path(row, base_dir):\n",
    "    return os.path.join(base_dir, row[\"label\"], row[\"filename\"])\n",
    "\n",
    "train_df[\"image_path\"] = train_df.apply(\n",
    "    lambda r: build_image_path(r, UN_SPLIT_PATH), axis=1\n",
    ")\n",
    "\n",
    "val_df[\"image_path\"] = val_df.apply(\n",
    "    lambda r: build_image_path(r, UN_SPLIT_PATH), axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1425fd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_existing_images(df, unsplit_base):\n",
    "    def image_exists(row):\n",
    "        # Extract basename and class from current image_path\n",
    "        basename = os.path.basename(row[\"image_path\"])\n",
    "        # Split on '0', adjust this logic as needed for your class extraction!\n",
    "        class_name = basename.split('0')[0]\n",
    "        candidate_path = os.path.join(unsplit_base, class_name, basename)\n",
    "        return os.path.exists(candidate_path)\n",
    "    \n",
    "    mask = df.apply(image_exists, axis=1)\n",
    "    return df[mask].reset_index(drop=True)\n",
    "\n",
    "# Usage:\n",
    "train_df = filter_existing_images(train_df, UN_SPLIT_PATH)\n",
    "val_df = filter_existing_images(val_df, UN_SPLIT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c78c15",
   "metadata": {},
   "source": [
    "## Neural Network Archetecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "51c0ad5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate, Flatten\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2484ce73",
   "metadata": {},
   "source": [
    "Let's start by defining the two input branches of our neural network: one for numerical data and another for image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f9b62763",
   "metadata": {},
   "outputs": [],
   "source": [
    "librosa_shape = (len(FEATURE_COLS),)\n",
    "image_shape = (432,288,3)\n",
    "\n",
    "# Input 1: e.g., a numerical feature vector (shape depends on your data)\n",
    "input1 = Input(librosa_shape, name='librosa_input') \n",
    "\n",
    "# Input 2: e.g., image data (shape depends on your image dimensions and channels)\n",
    "input2 = Input(image_shape, name='image_input') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8367ee3",
   "metadata": {},
   "source": [
    "Next we can create specificic branches for each input type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "162d23e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Branch 1 for numerical data\n",
    "x1 = Dense(32, activation='relu')(input1)\n",
    "x1 = Dense(16, activation='relu')(x1)\n",
    "\n",
    "# Branch 2 for image data (using placeholder layers for illustration)\n",
    "x2 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu')(input2)\n",
    "x2 = tf.keras.layers.MaxPooling2D((2, 2))(x2)\n",
    "x2 = Flatten()(x2) # Flatten the output of the CNN branch\n",
    "x2 = Dense(16, activation='relu')(x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0303342b",
   "metadata": {},
   "source": [
    "The next step is to combine the outputs of these branches and add some dense layers to process the combined information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3cb725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the outputs of the two branches\n",
    "concatenated = Concatenate()([x1, x2])\n",
    "\n",
    "# Add more layers after concatenation as needed\n",
    "y = Dense(32, activation='relu')(concatenated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00218205",
   "metadata": {},
   "source": [
    "Finally we need to define the output layer of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9f3a4cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final output layer\n",
    "output = Dense(1, activation='linear')(y) # Example for a regression task\n",
    "\n",
    "# Create the final model\n",
    "model = Model(inputs=[input1, input2], outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a27f2329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ image_input         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">432</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">288</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">430</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">286</span>,  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> │ image_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ librosa_input       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">58</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">215</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">143</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,888</span> │ librosa_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">491920</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │  <span style=\"color: #00af00; text-decoration-color: #00af00\">7,870,736</span> │ flatten[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,056</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │ dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ image_input         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m432\u001b[0m, \u001b[38;5;34m288\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m430\u001b[0m, \u001b[38;5;34m286\u001b[0m,  │        \u001b[38;5;34m448\u001b[0m │ image_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│                     │ \u001b[38;5;34m16\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ librosa_input       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m58\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m215\u001b[0m, \u001b[38;5;34m143\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ conv2d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m16\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │      \u001b[38;5;34m1,888\u001b[0m │ librosa_input[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m491920\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ max_pooling2d[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │        \u001b[38;5;34m528\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │  \u001b[38;5;34m7,870,736\u001b[0m │ flatten[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │      \u001b[38;5;34m1,056\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m33\u001b[0m │ dense_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,874,689</span> (30.04 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m7,874,689\u001b[0m (30.04 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,874,689</span> (30.04 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m7,874,689\u001b[0m (30.04 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273c0041",
   "metadata": {},
   "source": [
    "## Training Preliminary Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36e5a6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from datetime import datetime as datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bac995b",
   "metadata": {},
   "source": [
    "Now that we have an archetecture defined, we can prepare for training the model. This includes compiling the model with an appropriate loss function and optimizer. These steps are the same as the standard model training process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4d1314",
   "metadata": {},
   "source": [
    "Let's start by defining some base paths that we will use for loading and saving data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ef64cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = r\"C:\\Users\\JTWit\\Documents\\ECE 579\\Datasets\\Split GTZAN Dataset\"\n",
    "TEST_PATH = os.path.join(BASE_PATH,'test')\n",
    "TRAIN_PATH = os.path.join(BASE_PATH,'train')\n",
    "\n",
    "SAVE_PATH = os.path.join(r\"C:\\Users\\JTWit\\Documents\\ECE 579\",\"Custom DNN Models\")\n",
    "\n",
    "#Make the save path for the neural network just in case it does not yet exist\n",
    "os.makedirs(SAVE_PATH,exist_ok = True)\n",
    "\n",
    "checkpoint_dir = os.path.join(r\"C:\\Users\\JTWit\\Documents\\ECE 579\",'Training Checkpoints')\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}.weights.h5\")\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fac7a5a",
   "metadata": {},
   "source": [
    "Next, we will compile the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "749b20d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba81a6df",
   "metadata": {},
   "source": [
    "We can also add some optimizers and metrics to monitor during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "60f11648",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.2, \n",
    "    patience=2)\n",
    "\n",
    "earlystop = EarlyStopping(\n",
    "    monitor='val_acc',\n",
    "    mode=\"max\", \n",
    "    patience=3)\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True,  \n",
    "    monitor='val_loss',      \n",
    "    save_best_only=False,    \n",
    "    verbose=1                \n",
    ")\n",
    "\n",
    "\n",
    "callbacks = [reduce_lr,earlystop] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bb4139",
   "metadata": {},
   "source": [
    "We should now specify some of the hyperparameters for training the model, such as the number of epochs, thie initial learning rate, and batch size. We will also set the target size for the images and the name that we will use to save the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a933a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-5\n",
    "EPOCHS = 30\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "TARGET_SIZE = (432,288)\n",
    "\n",
    "NETWORK_NAME = \"GTZAN Multi-Input\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362412fc",
   "metadata": {},
   "source": [
    "Next, let's define the function to train the model using both types of input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f660986",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = (432, 288)\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "def load_sample(image_path, num_features, label):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_png(img, channels=3)\n",
    "    img = tf.image.resize(img, IMG_SIZE)\n",
    "    img = tf.cast(img, tf.float32) / 255.0\n",
    "    return (num_features, img), label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12f31232",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(df, training=True):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            df[\"image_path\"].values,\n",
    "            df[FEATURE_COLS].values.astype(\"float32\"),\n",
    "            df[\"label_id\"].values\n",
    "        )\n",
    "    )\n",
    "\n",
    "    if training:\n",
    "        ds = ds.shuffle(1024)\n",
    "\n",
    "    ds = ds.map(load_sample, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds = ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5e53c8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = make_dataset(train_df, training=True)\n",
    "val_ds   = make_dataset(val_df, training=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389fe956",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d95420",
   "metadata": {},
   "source": [
    "Now that we are finished with the preliminary work, we can train the model using the training data generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e7d5c6bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\JTWit\\AppData\\Local\\spyder-6\\envs\\ECE579\\Lib\\site-packages\\keras\\src\\losses\\losses.py:33: SyntaxWarning: In loss categorical_crossentropy, expected y_pred.shape to be (batch_size, num_classes) with num_classes > 1. Received: y_pred.shape=(None, 1). Consider using 'binary_crossentropy' if you only have 2 classes.\n",
      "  return self.fn(y_true, y_pred, **self._fn_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 286ms/step - accuracy: 0.0751 - loss: 5.3637e-07 - val_accuracy: 0.0850 - val_loss: 5.3644e-07 - learning_rate: 0.0010\n",
      "Epoch 2/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\JTWit\\AppData\\Local\\spyder-6\\envs\\ECE579\\Lib\\site-packages\\keras\\src\\callbacks\\early_stopping.py:99: UserWarning: Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: accuracy,loss,val_accuracy,val_loss,learning_rate\n",
      "  current = self.get_monitor_value(logs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 282ms/step - accuracy: 0.0751 - loss: 5.3637e-07 - val_accuracy: 0.0850 - val_loss: 5.3644e-07 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 266ms/step - accuracy: 0.0751 - loss: 5.3637e-07 - val_accuracy: 0.0850 - val_loss: 5.3644e-07 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 198ms/step - accuracy: 0.0751 - loss: 5.3637e-07 - val_accuracy: 0.0850 - val_loss: 5.3644e-07 - learning_rate: 2.0000e-04\n",
      "Epoch 5/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 199ms/step - accuracy: 0.0751 - loss: 5.3637e-07 - val_accuracy: 0.0850 - val_loss: 5.3644e-07 - learning_rate: 2.0000e-04\n",
      "Epoch 6/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 207ms/step - accuracy: 0.0751 - loss: 5.3637e-07 - val_accuracy: 0.0850 - val_loss: 5.3644e-07 - learning_rate: 4.0000e-05\n",
      "Epoch 7/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 189ms/step - accuracy: 0.0751 - loss: 5.3637e-07 - val_accuracy: 0.0850 - val_loss: 5.3644e-07 - learning_rate: 4.0000e-05\n",
      "Epoch 8/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 182ms/step - accuracy: 0.0751 - loss: 5.3637e-07 - val_accuracy: 0.0850 - val_loss: 5.3644e-07 - learning_rate: 8.0000e-06\n",
      "Epoch 9/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 199ms/step - accuracy: 0.0751 - loss: 5.3637e-07 - val_accuracy: 0.0850 - val_loss: 5.3644e-07 - learning_rate: 8.0000e-06\n",
      "Epoch 10/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 190ms/step - accuracy: 0.0751 - loss: 5.3637e-07 - val_accuracy: 0.0850 - val_loss: 5.3644e-07 - learning_rate: 1.6000e-06\n",
      "Epoch 11/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 187ms/step - accuracy: 0.0751 - loss: 5.3637e-07 - val_accuracy: 0.0850 - val_loss: 5.3644e-07 - learning_rate: 1.6000e-06\n",
      "Epoch 12/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 205ms/step - accuracy: 0.0751 - loss: 5.3637e-07 - val_accuracy: 0.0850 - val_loss: 5.3644e-07 - learning_rate: 3.2000e-07\n",
      "Epoch 13/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 201ms/step - accuracy: 0.0751 - loss: 5.3637e-07 - val_accuracy: 0.0850 - val_loss: 5.3644e-07 - learning_rate: 3.2000e-07\n",
      "Epoch 14/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 184ms/step - accuracy: 0.0751 - loss: 5.3637e-07 - val_accuracy: 0.0850 - val_loss: 5.3644e-07 - learning_rate: 6.4000e-08\n",
      "Epoch 15/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 187ms/step - accuracy: 0.0751 - loss: 5.3637e-07 - val_accuracy: 0.0850 - val_loss: 5.3644e-07 - learning_rate: 6.4000e-08\n",
      "Epoch 16/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 191ms/step - accuracy: 0.0751 - loss: 5.3637e-07 - val_accuracy: 0.0850 - val_loss: 5.3644e-07 - learning_rate: 1.2800e-08\n",
      "Epoch 17/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 182ms/step - accuracy: 0.0751 - loss: 5.3637e-07 - val_accuracy: 0.0850 - val_loss: 5.3644e-07 - learning_rate: 1.2800e-08\n",
      "Epoch 18/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 180ms/step - accuracy: 0.0751 - loss: 5.3637e-07 - val_accuracy: 0.0850 - val_loss: 5.3644e-07 - learning_rate: 2.5600e-09\n",
      "Epoch 19/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 183ms/step - accuracy: 0.0751 - loss: 5.3637e-07 - val_accuracy: 0.0850 - val_loss: 5.3644e-07 - learning_rate: 2.5600e-09\n",
      "Epoch 20/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 180ms/step - accuracy: 0.0751 - loss: 5.3637e-07 - val_accuracy: 0.0850 - val_loss: 5.3644e-07 - learning_rate: 5.1200e-10\n",
      "Epoch 21/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 181ms/step - accuracy: 0.0751 - loss: 5.3637e-07 - val_accuracy: 0.0850 - val_loss: 5.3644e-07 - learning_rate: 5.1200e-10\n",
      "Epoch 22/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 180ms/step - accuracy: 0.0751 - loss: 5.3637e-07 - val_accuracy: 0.0850 - val_loss: 5.3644e-07 - learning_rate: 1.0240e-10\n",
      "Epoch 23/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 183ms/step - accuracy: 0.0751 - loss: 5.3637e-07 - val_accuracy: 0.0850 - val_loss: 5.3644e-07 - learning_rate: 1.0240e-10\n",
      "Epoch 24/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 189ms/step - accuracy: 0.0751 - loss: 5.3637e-07 - val_accuracy: 0.0850 - val_loss: 5.3644e-07 - learning_rate: 2.0480e-11\n",
      "Epoch 25/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 190ms/step - accuracy: 0.0751 - loss: 5.3637e-07 - val_accuracy: 0.0850 - val_loss: 5.3644e-07 - learning_rate: 2.0480e-11\n",
      "Epoch 26/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 198ms/step - accuracy: 0.0751 - loss: 5.3637e-07 - val_accuracy: 0.0850 - val_loss: 5.3644e-07 - learning_rate: 4.0960e-12\n",
      "Epoch 27/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 215ms/step - accuracy: 0.0751 - loss: 5.3637e-07 - val_accuracy: 0.0850 - val_loss: 5.3644e-07 - learning_rate: 4.0960e-12\n",
      "Epoch 28/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 195ms/step - accuracy: 0.0751 - loss: 5.3637e-07 - val_accuracy: 0.0850 - val_loss: 5.3644e-07 - learning_rate: 8.1920e-13\n",
      "Epoch 29/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 185ms/step - accuracy: 0.0751 - loss: 5.3637e-07 - val_accuracy: 0.0850 - val_loss: 5.3644e-07 - learning_rate: 8.1920e-13\n",
      "Epoch 30/30\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 184ms/step - accuracy: 0.0751 - loss: 5.3637e-07 - val_accuracy: 0.0850 - val_loss: 5.3644e-07 - learning_rate: 1.6384e-13\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "accuracy = history.history['accuracy'][-1]\n",
    "date_str = datetime.today().strftime('%Y-%m-%d')\n",
    "name_string = f\"{NETWORK_NAME}(accuracy = {accuracy:.4f})(date = {date_str}).keras\"\n",
    "\n",
    "save_file = os.path.join(SAVE_PATH,name_string)\n",
    "\n",
    "model.save(save_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d644047",
   "metadata": {},
   "source": [
    "## Evaluating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4da0405",
   "metadata": {},
   "source": [
    "Now that we have trained the model, we can evaluate its performance on a test dataset containing both numerical and image data. Let's start by making a generator that we can use to load the test data in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2548a73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3829393b",
   "metadata": {},
   "source": [
    "## Scratch Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "013832d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4322fbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "LIBROSA_DATA = r\"C:\\Users\\JTWit\\Documents\\ECE 579\\Datasets\\GTZAN Dataset\\features_30_sec.csv\"\n",
    "\n",
    "BASE_PATH = r\"C:\\Users\\JTWit\\Documents\\ECE 579\\Datasets\\Split GTZAN Dataset\"\n",
    "TEST_PATH = os.path.join(BASE_PATH,'test')\n",
    "TRAIN_PATH = os.path.join(BASE_PATH,'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ef36a57d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3500881195068359 0.0887565687298774 0.1302279233932495\n",
      " 0.0028266964945942 1784.165849538755 129774.06452515082\n",
      " 2002.4490601176965 85882.76131549841 3805.8396058403423 901505.425532842\n",
      " 0.0830448206689868 0.000766945654594 -4.5297241740627214e-05\n",
      " 0.0081722820177674 7.783231922076084e-06 0.0056981821544468 123.046875\n",
      " -113.57064819335938 2564.20751953125 121.57179260253906 295.913818359375\n",
      " -19.168142318725582 235.57443237304688 42.36642074584961\n",
      " 151.10687255859375 -6.364664077758789 167.93479919433594\n",
      " 18.623498916625977 89.18083953857422 -13.704891204833984\n",
      " 67.66049194335938 15.34315013885498 68.93257904052734 -12.274109840393066\n",
      " 82.2042007446289 10.976572036743164 63.38631057739258 -8.326573371887207\n",
      " 61.773094177246094 8.803791999816895 51.24412536621094 -3.672300100326538\n",
      " 41.21741485595703 5.747994899749756 40.55447769165039 -5.162881851196289\n",
      " 49.775421142578125 0.752740204334259 52.4209098815918 -1.6902146339416504\n",
      " 36.524070739746094 -0.4089791774749756 41.59710311889648\n",
      " -2.3035225868225098 55.06292343139648 1.2212907075881958 46.93603515625]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(LIBROSA_DATA)\n",
    "\n",
    "df['image_name'] = df['filename'].str.replace('.','',1)\n",
    "df['image_name'] = df['image_name'].str.replace('.wav','.png')\n",
    "\n",
    "arr = df.loc[df['image_name'] == 'blues00000.png'].values\n",
    "\n",
    "print(arr.reshape(-1)[2:59])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18364db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(799,)\n"
     ]
    }
   ],
   "source": [
    "X_TRAIN = list()\n",
    "\n",
    "for root, dirs, files in os.walk(TRAIN_PATH):\n",
    "    for dir in dirs:\n",
    "\n",
    "        temp_dict = {}\n",
    "        for file in os.listdir(os.path.join(root,dir)):\n",
    "            \n",
    "\n",
    "            file_path = os.path.join(root,dir,file)\n",
    "            temp_dict['image'] = cv2.imread(file_path)\n",
    "            temp_dict['librosa_data'] = (df.loc[df['image_name'] == file].values).reshape(-1)[2:59]\n",
    "            temp_dict['label'] = dir\n",
    "\n",
    "            X_TRAIN.append(temp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1809c208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(799,)\n",
      "dict_keys(['image', 'librosa_data', 'label'])\n"
     ]
    }
   ],
   "source": [
    "X_TRAIN = np.array(X_TRAIN)\n",
    "print(X_TRAIN.shape)\n",
    "print(X_TRAIN[0].keys())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ECE579",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
