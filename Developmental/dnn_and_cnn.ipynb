{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7101cfd3",
   "metadata": {},
   "source": [
    "# Deep and Convolutional Neural Networks for Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6893731c",
   "metadata": {},
   "source": [
    "At surface value there is not much difference between deep neural networks (DNNs) and convolutional neural networks (CNNs). Both are types of artificial neural networks used in machine learning and deep learning. However, they have different architectures and are suited for different types of tasks.\n",
    "\n",
    "DNNs are composed of multiple layers of interconnected neurons, where each neuron in one layer is connected to every neuron in the next layer. This architecture allows DNNs to learn complex patterns and relationships in data, making them suitable for a wide range of tasks, including image classification, natural language processing, and speech recognition.\n",
    "\n",
    "CNNs, on the other hand, are specifically designed for processing grid-like data, such as images. They use convolutional layers that apply filters to the input data to extract features, followed by pooling layers that reduce the spatial dimensions of the data. This architecture allows CNNs to effectively capture spatial hierarchies and patterns in images, making them particularly well-suited for image classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba9607e",
   "metadata": {},
   "source": [
    "## Network Architecture Design"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82168e0",
   "metadata": {},
   "source": [
    "Starting with architecture, there are a number of layers that are commonly used in DNNs and CNNs. For DNNs, these include:\n",
    "- Input Layer: The first layer that receives the input data.\n",
    "- Hidden Layers: Multiple layers of neurons that process the input data and learn patterns.\n",
    "- Output Layer: The final layer that produces the output predictions.\n",
    "For CNNs, the common layers include:\n",
    "- Convolutional Layers: Apply filters to the input data to extract features.\n",
    "- Pooling Layers: Reduce the spatial dimensions of the data.\n",
    "- Fully Connected Layers: Similar to the hidden layers in DNNs, where each neuron is connected to every neuron in the next layer.\n",
    "- Output Layer: The final layer that produces the output predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6201af2",
   "metadata": {},
   "source": [
    "We will use tensorflow and keras to build both types of networks for image classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7ed9375",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input, AveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fbb34059",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The size of the input images\n",
    "input_shape = (432,288,3)\n",
    "\n",
    "#The number of classes\n",
    "classes_count = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce5429ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Input(input_shape),\n",
    "\n",
    "    # First convolutional layer\n",
    "    Conv2D(32,kernel_size=(5,5),activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "    # Second convolutional layer\n",
    "    Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "    # Flatten the feature map\n",
    "    Flatten(),\n",
    "    \n",
    "    # Fully connected layers\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "\n",
    "    Dense(64, activation='relu'),\n",
    "    \n",
    "    # Output layer\n",
    "    Dense(classes_count, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6995ef7a",
   "metadata": {},
   "source": [
    "We can print a summary of the model architecture to see the number of layers and parameters in each network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "50ff7dbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">428</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">284</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,432</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">214</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">142</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">212</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">140</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">106</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">70</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">474880</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │    <span style=\"color: #00af00; text-decoration-color: #00af00\">60,784,768</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m428\u001b[0m, \u001b[38;5;34m284\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │         \u001b[38;5;34m2,432\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m214\u001b[0m, \u001b[38;5;34m142\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m212\u001b[0m, \u001b[38;5;34m140\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m106\u001b[0m, \u001b[38;5;34m70\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m474880\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │    \u001b[38;5;34m60,784,768\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m650\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">60,814,602</span> (231.99 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m60,814,602\u001b[0m (231.99 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">60,814,602</span> (231.99 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m60,814,602\u001b[0m (231.99 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f39ddc",
   "metadata": {},
   "source": [
    "## Network Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b89031e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from datetime import datetime as datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c5d9de",
   "metadata": {},
   "source": [
    "We will need to configure a couple of things before we dive into training the neural network. We need to specify where the training and testing data is located, the number of classes we are trying to predict, the image size, and the batch size. These parameters will be used to configure the data generators as well as the neural network architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "718e4687",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = r\"C:\\Users\\JTWit\\Documents\\ECE 579\\Datasets\\Split GTZAN Dataset\"\n",
    "TEST_PATH = os.path.join(BASE_PATH,'test')\n",
    "TRAIN_PATH = os.path.join(BASE_PATH,'train')\n",
    "\n",
    "SAVE_PATH = os.path.join(r\"C:\\Users\\JTWit\\Documents\\ECE 579\",\"Custom DNN Models\")\n",
    "\n",
    "#Make the save path for the neural network just in case it does not yet exist\n",
    "os.makedirs(SAVE_PATH,exist_ok = True)\n",
    "\n",
    "checkpoint_dir = os.path.join(r\"C:\\Users\\JTWit\\Documents\\ECE 579\",'Training Checkpoints')\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}.weights.h5\")\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "642bccae",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-5\n",
    "EPOCHS = 30\n",
    "\n",
    "TARGET_SIZE = (432,288)\n",
    "\n",
    "NETWORK_NAME = \"GTZAN Custom DNN\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcafa1b5",
   "metadata": {},
   "source": [
    "Let's specify some callbacks to help with training. Callbacks are functions that are called during the training process at specific points, such as at the end of an epoch or after a certain number of batches. They can be used to monitor the training process, save the model, and adjust the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7202baf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.2, \n",
    "    patience=2)\n",
    "\n",
    "earlystop = EarlyStopping(\n",
    "    monitor='val_acc',\n",
    "    mode=\"max\", \n",
    "    patience=3)\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True,  \n",
    "    monitor='val_loss',      \n",
    "    save_best_only=False,    \n",
    "    verbose=1                \n",
    ")\n",
    "\n",
    "\n",
    "callbacks = [reduce_lr,earlystop] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418cd4b0",
   "metadata": {},
   "source": [
    "With everything in place, we can now compile the models and begin training. We will use the Adam optimizer and categorical cross-entropy loss function for both networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e8621fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26342d6",
   "metadata": {},
   "source": [
    "We will also have to configure the train and validation data generators to load the images from the specified directories, resize them to the desired input shape, and apply any necessary preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "54cb243d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_and_validation_data(training_path, training_options, validation_split=0.2):\n",
    "    # Create an ImageDataGenerator with validation_split\n",
    "    datagen = ImageDataGenerator(\n",
    "        rotation_range=training_options[\"rotation_range\"],\n",
    "        width_shift_range=training_options[\"width_shift_range\"],\n",
    "        height_shift_range=training_options[\"height_shift_range\"],\n",
    "        brightness_range=training_options[\"brightness_range\"],\n",
    "        rescale=1./255,  # Important for scaling pixel values\n",
    "        validation_split=validation_split\n",
    "    )\n",
    "\n",
    "    # Training generator\n",
    "    train_generator = datagen.flow_from_directory(\n",
    "        training_path,\n",
    "        target_size=training_options[\"target_size\"],\n",
    "        batch_size=training_options[\"batch_size\"],\n",
    "        class_mode='categorical',\n",
    "        subset='training'\n",
    "    )\n",
    "\n",
    "    # Validation generator\n",
    "    validation_generator = datagen.flow_from_directory(\n",
    "        training_path,\n",
    "        target_size=training_options[\"target_size\"],\n",
    "        batch_size=training_options[\"batch_size\"],\n",
    "        class_mode='categorical',\n",
    "        subset='validation'\n",
    "    )\n",
    "\n",
    "    return train_generator, validation_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "48bc0413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 640 images belonging to 10 classes.\n",
      "Found 159 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "#Data generators\n",
    "train_options = {\n",
    "    \"rotation_range\": 0,           # Slightly reduced\n",
    "    \"width_shift_range\": 0.0,      # Up to 5% shift (1.5 pixels for 30x30)\n",
    "    \"height_shift_range\": 0.,     # Up to 5% shift\n",
    "    \"brightness_range\": (1, 1), # Gentle brightness adjustment\n",
    "    \"target_size\": TARGET_SIZE,\n",
    "    \"batch_size\": 16\n",
    "}\n",
    "\n",
    "train_gen,valid_gen = get_train_and_validation_data(TRAIN_PATH,train_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0487ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 1s/step - accuracy: 0.8906 - loss: 0.3229 - val_accuracy: 0.4969 - val_loss: 2.2162 - learning_rate: 0.0010\n",
      "Epoch 2/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\JTWit\\AppData\\Local\\spyder-6\\envs\\ECE579\\Lib\\site-packages\\keras\\src\\callbacks\\early_stopping.py:99: UserWarning: Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: accuracy,loss,val_accuracy,val_loss,learning_rate\n",
      "  current = self.get_monitor_value(logs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 1s/step - accuracy: 0.9141 - loss: 0.2424 - val_accuracy: 0.4591 - val_loss: 2.3171 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 1s/step - accuracy: 0.9312 - loss: 0.2280 - val_accuracy: 0.4340 - val_loss: 2.5203 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 1s/step - accuracy: 0.9359 - loss: 0.1889 - val_accuracy: 0.4717 - val_loss: 2.4472 - learning_rate: 2.0000e-04\n",
      "Epoch 5/30\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 2s/step - accuracy: 0.9609 - loss: 0.1326 - val_accuracy: 0.4780 - val_loss: 2.5027 - learning_rate: 2.0000e-04\n",
      "Epoch 6/30\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 1s/step - accuracy: 0.9672 - loss: 0.1076 - val_accuracy: 0.4906 - val_loss: 2.5540 - learning_rate: 4.0000e-05\n",
      "Epoch 7/30\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 1s/step - accuracy: 0.9594 - loss: 0.1190 - val_accuracy: 0.5094 - val_loss: 2.6274 - learning_rate: 4.0000e-05\n",
      "Epoch 8/30\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 1s/step - accuracy: 0.9422 - loss: 0.1615 - val_accuracy: 0.5031 - val_loss: 2.6213 - learning_rate: 8.0000e-06\n",
      "Epoch 9/30\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 1s/step - accuracy: 0.9703 - loss: 0.0828 - val_accuracy: 0.5031 - val_loss: 2.6037 - learning_rate: 8.0000e-06\n",
      "Epoch 10/30\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 1s/step - accuracy: 0.9547 - loss: 0.1255 - val_accuracy: 0.5094 - val_loss: 2.6048 - learning_rate: 1.6000e-06\n",
      "Epoch 11/30\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 1s/step - accuracy: 0.9594 - loss: 0.1123 - val_accuracy: 0.5094 - val_loss: 2.6038 - learning_rate: 1.6000e-06\n",
      "Epoch 12/30\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 1s/step - accuracy: 0.9547 - loss: 0.1257 - val_accuracy: 0.5094 - val_loss: 2.6026 - learning_rate: 3.2000e-07\n",
      "Epoch 13/30\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 1s/step - accuracy: 0.9531 - loss: 0.1243 - val_accuracy: 0.5094 - val_loss: 2.6024 - learning_rate: 3.2000e-07\n",
      "Epoch 14/30\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 1s/step - accuracy: 0.9453 - loss: 0.1377 - val_accuracy: 0.5094 - val_loss: 2.6024 - learning_rate: 6.4000e-08\n",
      "Epoch 15/30\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 1s/step - accuracy: 0.9578 - loss: 0.1299 - val_accuracy: 0.5094 - val_loss: 2.6023 - learning_rate: 6.4000e-08\n",
      "Epoch 16/30\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 1s/step - accuracy: 0.9641 - loss: 0.1036 - val_accuracy: 0.5094 - val_loss: 2.6023 - learning_rate: 1.2800e-08\n",
      "Epoch 17/30\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 1s/step - accuracy: 0.9656 - loss: 0.1001 - val_accuracy: 0.5094 - val_loss: 2.6022 - learning_rate: 1.2800e-08\n",
      "Epoch 18/30\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 1s/step - accuracy: 0.9594 - loss: 0.1580 - val_accuracy: 0.5094 - val_loss: 2.6022 - learning_rate: 2.5600e-09\n",
      "Epoch 19/30\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 1s/step - accuracy: 0.9672 - loss: 0.1107 - val_accuracy: 0.5094 - val_loss: 2.6022 - learning_rate: 2.5600e-09\n",
      "Epoch 20/30\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 1s/step - accuracy: 0.9422 - loss: 0.1380 - val_accuracy: 0.5094 - val_loss: 2.6022 - learning_rate: 5.1200e-10\n",
      "Epoch 21/30\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 1s/step - accuracy: 0.9625 - loss: 0.1307 - val_accuracy: 0.5094 - val_loss: 2.6022 - learning_rate: 5.1200e-10\n",
      "Epoch 22/30\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 1s/step - accuracy: 0.9609 - loss: 0.1084 - val_accuracy: 0.5094 - val_loss: 2.6022 - learning_rate: 1.0240e-10\n",
      "Epoch 23/30\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1s/step - accuracy: 0.9703 - loss: 0.1024 - val_accuracy: 0.5094 - val_loss: 2.6022 - learning_rate: 1.0240e-10\n",
      "Epoch 24/30\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 2s/step - accuracy: 0.9750 - loss: 0.1066 - val_accuracy: 0.5094 - val_loss: 2.6022 - learning_rate: 2.0480e-11\n",
      "Epoch 25/30\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 1s/step - accuracy: 0.9563 - loss: 0.1024 - val_accuracy: 0.5094 - val_loss: 2.6022 - learning_rate: 2.0480e-11\n",
      "Epoch 26/30\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 2s/step - accuracy: 0.9719 - loss: 0.1016 - val_accuracy: 0.5094 - val_loss: 2.6022 - learning_rate: 4.0960e-12\n",
      "Epoch 27/30\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 1s/step - accuracy: 0.9672 - loss: 0.0983 - val_accuracy: 0.5094 - val_loss: 2.6022 - learning_rate: 4.0960e-12\n",
      "Epoch 28/30\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 1s/step - accuracy: 0.9547 - loss: 0.1308 - val_accuracy: 0.5094 - val_loss: 2.6022 - learning_rate: 8.1920e-13\n",
      "Epoch 29/30\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 1s/step - accuracy: 0.9531 - loss: 0.1201 - val_accuracy: 0.5094 - val_loss: 2.6022 - learning_rate: 8.1920e-13\n",
      "Epoch 30/30\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 1s/step - accuracy: 0.9625 - loss: 0.1056 - val_accuracy: 0.5094 - val_loss: 2.6022 - learning_rate: 1.6384e-13\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_gen, validation_data=valid_gen, epochs=EPOCHS, callbacks =  callbacks)\n",
    "    \n",
    "accuracy = history.history['accuracy'][-1]\n",
    "date_str = datetime.today().strftime('%Y-%m-%d')\n",
    "name_string = f\"{NETWORK_NAME}(accuracy = {accuracy:.4f})(date = {date_str}).keras\"\n",
    "\n",
    "save_file = os.path.join(SAVE_PATH,name_string)\n",
    "\n",
    "model.save(save_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77699d3",
   "metadata": {},
   "source": [
    "## Network Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd012f6",
   "metadata": {},
   "source": [
    "With a trained neural networkk in hand, we can evaluate its performance on the test dataset. This involves using the model to make predictions on the test data and comparing those predictions to the true labels to calculate metrics such as accuracy, precision, recall, and F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a55e40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ECE579",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
